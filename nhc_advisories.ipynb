{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "e9a372ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the latest NHC advisories (TEXT rather than a-decks since they are going to be updated faster)\n",
    "# parse them to recreate the forecast intensities\n",
    "# use the CSV tables from nhc_errors to produce probabilities (RI or not)\n",
    "# ONLY AL AND EP FOR NOW\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pytz\n",
    "import pandas as pd\n",
    "# to create word numbers to numbers mapping\n",
    "import inflect\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# FORECAST ADVISORY TEXT BASIN NAMES (THESE ARE THE ABBREVS USED IN FOLDER NAMES) ('AT' WILL BECOME 'AL' in df):\n",
    "basin_initials = ['AT', 'EP', 'CP']\n",
    "# currently 5 bins to put active storms (may increase to 10 in future)\n",
    "num_bins = 5\n",
    "\n",
    "nhc_text_url_base = 'https://www.nhc.noaa.gov/archive/text'\n",
    "\n",
    "# AL AND EP csv (tables) for percentiles (created from nhc_errors.ipynb)\n",
    "nhc_error_tables = ['table1.csv',\n",
    " 'table2.csv',\n",
    " 'table3.csv',\n",
    " 'table4.csv',\n",
    " 'table5.csv',\n",
    " 'table6.csv',\n",
    " 'table7.csv',\n",
    " 'table8.csv',\n",
    " 'table9.csv',\n",
    " 'table10.csv']\n",
    "\n",
    "# when calculating RI adjusted probabilities, limit calculations with RI up to tau (low samples)\n",
    "max_tau_by_basin = {\n",
    "    'AL': 36,\n",
    "    'EP': 72\n",
    "}\n",
    "\n",
    "# Based on \"INITIAL THOUGHTS\" sections in nhc_errors.ipynb, that conditionalizes on whether RI is in forecast\n",
    "#   each RI, NO_RI is a list of tuples:\n",
    "#      the first value is the table # (as it appears in nhc_errors), and\n",
    "#      the second value is the weight for the corresponding probability\n",
    "adjust_RI_weights = {\n",
    "    'AL': {\n",
    "        'RI': [(4, 0.0275), (1, 0.9725)],\n",
    "        'NO_RI': [(3, 0.6679), (2, 0.3321)]\n",
    "    },\n",
    "    'EP': {\n",
    "        'RI': [(9, 0.0184), (6, 0.9816)],\n",
    "        'NO_RI': [(8, 0.6225), (7, 0.3375)]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157c2ae5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "45cc3f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saffir-Simpson (in kt)\n",
    "saffir_categories_kt = {\n",
    "    'TD': (0, 33),\n",
    "    'TS': (34, 63),\n",
    "    'CAT1': (64, 82),\n",
    "    'CAT2': (83, 95),\n",
    "    'CAT3': (96, 112),\n",
    "    'CAT4': (113, 136),\n",
    "    'CAT5': (137, np.inf)\n",
    "}\n",
    "\n",
    "# NHC advisories only have increments of 5kt, so use categories that fit that\n",
    "advisory_categories_kt = {\n",
    "    'TD': (0, 30),\n",
    "    'TS': (35, 60),\n",
    "    'CAT1': (65, 80),\n",
    "    'CAT2': (85, 95),\n",
    "    'CAT3': (100, 110),\n",
    "    'CAT4': (115, 135),\n",
    "    'CAT5': (140, np.inf)\n",
    "}\n",
    "\n",
    "#######################################################################\n",
    "# the choice of which of the above to use does alter the probabilities\n",
    "#######################################################################\n",
    "categories_kt = advisory_categories_kt\n",
    "\n",
    "p = inflect.engine()\n",
    "word_names = []\n",
    "word_names_to_numbers = {}\n",
    "for i in range(1, 100):\n",
    "    word = p.number_to_words(i).lower()\n",
    "    word_names.append(word)\n",
    "    word_names_to_numbers[word] = i\n",
    "\n",
    "month_abbrevs = ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']\n",
    "month_abbrevs_to_month_num = {v: i for i, v in enumerate(month_abbrevs, 1)}\n",
    "\n",
    "# initialize min, max names & percentiles for each category\n",
    "# initialize probs (based on percentiles)\n",
    "cat_min_max_kt = {}\n",
    "p_cat_labels = {}\n",
    "for cat, [cat_min, cat_max] in categories_kt.items():\n",
    "    cat_min_max_kt[f'{cat}_MIN'] = cat_min\n",
    "    cat_min_max_kt[f'{cat}_MAX'] = cat_max\n",
    "    p_cat_labels[cat] = f'P({cat})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "ce12b6c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_text_file_names(base_url):\n",
    "    response = requests.get(base_url)\n",
    "    time.sleep(1)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    urls = []\n",
    "    for link in soup.find_all('a'):\n",
    "        href = link.get('href').rstrip(\"/\")\n",
    "        if href and not href.startswith('../') and href.endswith('.txt'):\n",
    "            urls.append(href)\n",
    "    return urls\n",
    "\n",
    "def get_text_file(url):\n",
    "    response = requests.get(url)\n",
    "    time.sleep(1)\n",
    "    if response.status_code == 200:\n",
    "        return response.text\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# get urls of text folders for all basins\n",
    "def get_nhc_text_folder_urls(year):\n",
    "    year_str = str(year)\n",
    "    #https://www.nhc.noaa.gov/archive/text/TCMAT1\n",
    "    nhc_text_folders = []\n",
    "    for basin in basin_initials:\n",
    "        basin = basin.upper()\n",
    "        for bin_num in range(1, num_bins+1):\n",
    "            bin_num_str = str(bin_num)\n",
    "            url = f'{nhc_text_url_base}/TCM{basin}{bin_num_str}/{year_str}'\n",
    "            nhc_text_folders.append(url)\n",
    "    return nhc_text_folders\n",
    "\n",
    "# get for all basins\n",
    "def get_last_forecasts_text():\n",
    "    year = datetime.now().year\n",
    "    nhc_text_folder_urls = get_nhc_text_folder_urls(year)\n",
    "\n",
    "    if not nhc_text_folder_urls:\n",
    "        return None\n",
    "    \n",
    "    txts = []\n",
    "    for folder_url in nhc_text_folder_urls:\n",
    "        flist = get_text_file_names(folder_url)\n",
    "        if flist and len(flist) > 0:\n",
    "            last_txt_file_name = sorted(flist, reverse=True)[0]\n",
    "            url = f'{folder_url}/{last_txt_file_name}'\n",
    "            txt = get_text_file(url)\n",
    "            if txt:\n",
    "                txts.append(txt)\n",
    "    return txts\n",
    "\n",
    "def df_from_ofcl_forecast(txt):\n",
    "    #print(txt)\n",
    "    df_ofcl_forecast = pd.DataFrame({\n",
    "                        'ATCF_ID': pd.Series(dtype='str'),\n",
    "                        'BASIN': pd.Series(dtype='str'),\n",
    "                        'STORM_NUM': pd.Series(dtype='int'),\n",
    "                        'SEASON': pd.Series(dtype='str'),\n",
    "                        'NAME': pd.Series(dtype='str'),\n",
    "                        'INIT_TIME_UTC': pd.Series(dtype='datetime64[ns, UTC]'),\n",
    "                        'VALID_TIME_UTC': pd.Series(dtype='datetime64[ns, UTC]'),\n",
    "                        'TAU': pd.Series(dtype='int'),\n",
    "                        'VMAX_KT': pd.Series(dtype='float'),\n",
    "                        'STORM_TYPE': pd.Series(dtype='str')\n",
    "    })\n",
    "\n",
    "    lines = txt.split('\\n')\n",
    "    r_name = re.compile('(?P<init_storm_type>.*?) +(?P<storm_name>\\w+)(?:-[EACLPW])? +FORECAST/ADVISORY NUMBER +(?P<advisory_num>\\d+)')\n",
    "    r_atcf = re.compile('^NWS (?:NATIONAL HURRICANE CENTER MIAMI FL|CENTRAL PACIFIC HURRICANE CENTER HONOLULU HI) +(?P<atcf_id>\\w+)')\n",
    "    r_advisory_date = re.compile(f'^(?P<advisory_time>\\d\\d\\d\\d) UTC \\w+ +(?P<month_abbrev>\\w+) +(?P<advisory_day_num>\\d+) +(?P<advisory_year>\\d\\d\\d\\d)')\n",
    "    r_init_time = re.compile('^AT (?P<init_day_num>\\d+)/(?P<init_time_hhmm>\\d\\d\\d\\d)Z CENTER')\n",
    "    r_init_vmax = re.compile('^MAX SUSTAINED WINDS +(?P<init_vmax>\\d+) +KT')\n",
    "    r_valid_time_with_center = re.compile('^(?:FORECAST|OUTLOOK) VALID (?P<valid_day_num>\\d+)/(?P<valid_time_hhmm>\\d\\d\\d\\d)Z +[\\w\\.]+[NS] +[\\w\\.]+[WE]\\.*(?P<valid_storm_type>.*)')\n",
    "    r_valid_time_without_center = re.compile('^(?:FORECAST|OUTLOOK) VALID (?P<valid_day_num>\\d+)/(?P<valid_time_hhmm>\\d\\d\\d\\d)Z\\.*(?P<valid_storm_type>.*)')\n",
    "    r_valid_vmax = re.compile('^MAX WIND +(?P<valid_vmax>\\d+) +KT')\n",
    "    valid_datetime = None\n",
    "\n",
    "    ofcl_forecast = {}\n",
    "    \n",
    "    for line in lines:\n",
    "        res_name = re.match(r_name, line)\n",
    "        res_atcf = re.match(r_atcf, line)\n",
    "        res_advisory_date = re.match(r_advisory_date, line)\n",
    "        res_init_time = re.match(r_init_time, line)\n",
    "        res_init_vmax = re.match(r_init_vmax, line)\n",
    "        res_valid_time = re.match(r_valid_time_with_center, line)\n",
    "        if not res_valid_time:\n",
    "            res_valid_time = re.match(r_valid_time_without_center, line)\n",
    "        res_valid_vmax = re.match(r_valid_vmax, line)\n",
    "        if res_name:\n",
    "            init_storm_type = res_name['init_storm_type'].strip()\n",
    "            storm_name = res_name['storm_name'].strip()\n",
    "            advisory_num = int(res_name['advisory_num'].strip())\n",
    "            if storm_name.lower() in word_names:\n",
    "                # this is a (special) potential cyclone advisory as it has a number instead of a name\n",
    "                storm_name = str(word_names_to_numbers[storm_name.lower()])\n",
    "        elif res_atcf:\n",
    "            atcf_id = res_atcf['atcf_id'].strip()\n",
    "            atcf_basin = atcf_id[0:2]\n",
    "            afcf_storm_num = int(atcf_id[2:4])\n",
    "            afcf_season = int(atcf_id[4:8])\n",
    "        elif res_advisory_date:\n",
    "            advisory_time = res_advisory_date['advisory_time'].strip()\n",
    "            month_abbrev = res_advisory_date['month_abbrev'].strip().lower()\n",
    "            advisory_month_num = month_abbrevs_to_month_num[month_abbrev]\n",
    "            advisory_day_num = int(res_advisory_date['advisory_day_num'].strip())\n",
    "            advisory_year = int(res_advisory_date['advisory_year'].strip())\n",
    "            # we need the advisory date/day num for forecasts near midnight at the end/start of a month:\n",
    "            #   to check if init time are less than, or valid times are more than, the advisory time to adjust the month\n",
    "            advisory_date_str = f'{advisory_year}-{advisory_month_num:02}-{advisory_day_num:02} {advisory_time}'\n",
    "            advisory_datetime = datetime.strptime(advisory_date_str, \"%Y-%m-%d %H%M\").replace(tzinfo=pytz.utc)\n",
    "        elif res_init_time:\n",
    "            init_day_num = int(res_init_time['init_day_num'].strip())\n",
    "            init_time_hhmm = res_init_time['init_time_hhmm'].strip()\n",
    "            # check to see if same month\n",
    "            init_day_before_advisory = (init_day_num != (advisory_day_num - 1))\n",
    "            init_day_after_advisory = (init_day_num != (advisory_day_num + 1))\n",
    "            if init_day_before_advisory or init_day_after_advisory:\n",
    "                init_month_num = advisory_month_num\n",
    "                init_year = advisory_year\n",
    "            elif init_day_before_advisory:\n",
    "                # init is from previous month (relative to advisory time)\n",
    "                init_month_num = advisory_month_num - 1\n",
    "                if init_month_num == 0:\n",
    "                    init_month_num = 12\n",
    "                    init_year = advisory_year - 1\n",
    "                else:\n",
    "                    init_year = advisory_year\n",
    "            elif init_day_after_advisory:\n",
    "                # init is from previous month (relative to advisory time)\n",
    "                # this should never happen as usually the advisory time is after init time\n",
    "                init_month_num = advisory_month_num + 1\n",
    "                if init_month_num == 13:\n",
    "                    init_month_num = 1\n",
    "                    init_year = advisory_year + 1\n",
    "                else:\n",
    "                    init_year = advisory_year\n",
    "            init_date_str = f'{init_year}-{init_month_num:02}-{init_day_num:02} {init_time_hhmm}'\n",
    "            init_datetime = datetime.strptime(init_date_str, \"%Y-%m-%d %H%M\").replace(tzinfo=pytz.utc)\n",
    "\n",
    "            # init vmax comes before init time\n",
    "            ofcl_forecast[init_datetime] = init_vmax\n",
    "            tau = 0\n",
    "            row = [\n",
    "                    atcf_id,\n",
    "                    atcf_basin,\n",
    "                    afcf_storm_num,\n",
    "                    afcf_season,\n",
    "                    storm_name,\n",
    "                    init_datetime,\n",
    "                    init_datetime,\n",
    "                    tau,\n",
    "                    init_vmax,\n",
    "                    init_storm_type\n",
    "                ]\n",
    "            new_df = pd.DataFrame([row], columns=df_ofcl_forecast.columns)\n",
    "            df_ofcl_forecast = pd.concat([df_ofcl_forecast, new_df])\n",
    "        elif res_init_vmax:\n",
    "            init_vmax = int(res_init_vmax['init_vmax'].strip())\n",
    "        elif res_valid_time:\n",
    "            valid_day_num = int(res_valid_time['valid_day_num'].strip())\n",
    "            valid_time_hhmm = res_valid_time['valid_time_hhmm'].strip()\n",
    "            if 'valid_storm_type' in res_valid_time.groupdict():\n",
    "                valid_storm_type = res_valid_time['valid_storm_type'].strip()\n",
    "            else:\n",
    "                valid_storm_type = init_storm_type\n",
    "\n",
    "            # check to see if same month\n",
    "            valid_day_before_advisory = (valid_day_num != (advisory_day_num - 1))\n",
    "            valid_day_after_advisory = (valid_day_num != (advisory_day_num + 1))\n",
    "            if valid_day_before_advisory or valid_day_after_advisory:\n",
    "                valid_month_num = advisory_month_num\n",
    "                valid_year = advisory_year\n",
    "            elif valid_day_before_advisory:\n",
    "                # valid is from previous month (relative to advisory time)\n",
    "                valid_month_num = advisory_month_num - 1\n",
    "                if valid_month_num == 0:\n",
    "                    valid_month_num = 12\n",
    "                    valid_year = advisory_year - 1\n",
    "                else:\n",
    "                    valid_year = advisory_year\n",
    "            elif valid_day_after_advisory:\n",
    "                # valid is from previous month (relative to advisory time)\n",
    "                # this should never happen as usually the advisory time is after valid time\n",
    "                valid_month_num = advisory_month_num + 1\n",
    "                if valid_month_num == 13:\n",
    "                    valid_month_num = 1\n",
    "                    valid_year = advisory_year + 1\n",
    "                else:\n",
    "                    valid_year = advisory_year\n",
    "            valid_date_str = f'{valid_year}-{valid_month_num:02}-{valid_day_num:02} {valid_time_hhmm}'\n",
    "            valid_datetime = datetime.strptime(valid_date_str, \"%Y-%m-%d %H%M\").replace(tzinfo=pytz.utc)\n",
    "\n",
    "            if \"dissipat\" in valid_storm_type.lower():\n",
    "                tau = int((valid_datetime - init_datetime).total_seconds() / 3600)\n",
    "                valid_vmax = np.nan\n",
    "                ofcl_forecast[valid_datetime] = valid_vmax\n",
    "                row = [\n",
    "                    atcf_id,\n",
    "                    atcf_basin,\n",
    "                    afcf_storm_num,\n",
    "                    afcf_season,\n",
    "                    storm_name,\n",
    "                    init_datetime,\n",
    "                    valid_datetime,\n",
    "                    tau,\n",
    "                    valid_vmax,\n",
    "                    valid_storm_type\n",
    "                ]\n",
    "                new_df = pd.DataFrame([row], columns=df_ofcl_forecast.columns)\n",
    "                df_ofcl_forecast = pd.concat([df_ofcl_forecast, new_df])\n",
    "                # reset datetime\n",
    "                valid_datetime = None\n",
    "        elif res_valid_vmax:\n",
    "            valid_vmax = int(res_valid_vmax['valid_vmax'].strip())\n",
    "            # add valid to forecast\n",
    "            ofcl_forecast[valid_datetime] = valid_vmax\n",
    "            tau = int((valid_datetime - init_datetime).total_seconds() / 3600)\n",
    "            row = [\n",
    "                    atcf_id,\n",
    "                    atcf_basin,\n",
    "                    afcf_storm_num,\n",
    "                    afcf_season,\n",
    "                    storm_name,\n",
    "                    init_datetime,\n",
    "                    valid_datetime,\n",
    "                    tau,\n",
    "                    valid_vmax,\n",
    "                    valid_storm_type\n",
    "                ]\n",
    "            new_df = pd.DataFrame([row], columns=df_ofcl_forecast.columns)\n",
    "            df_ofcl_forecast = pd.concat([df_ofcl_forecast, new_df])\n",
    "            # reset datetime\n",
    "            valid_datetime = None\n",
    "    return df_ofcl_forecast\n",
    "\n",
    "# get for all basins\n",
    "def get_last_forecast_dfs():\n",
    "    # get the text for the last forecast for every bin of all basins\n",
    "    txts = get_last_forecasts_text()\n",
    "    df_ofcl_forecasts = None\n",
    "    for txt in txts:\n",
    "        try:\n",
    "            df_ofcl_forecast = df_from_ofcl_forecast(txt)\n",
    "            \n",
    "            if type(df_ofcl_forecasts) is type(None) or (\n",
    "                (type(df_ofcl_forecasts) is type(pd.DataFrame())) and df_ofcl_forecast.empty\n",
    "            ):\n",
    "                df_ofcl_forecasts = df_ofcl_forecast\n",
    "            else:\n",
    "                if (type(df_ofcl_forecast) is type(pd.DataFrame())) and not (df_ofcl_forecast.empty):\n",
    "                    df_ofcl_forecasts = pd.concat([df_ofcl_forecasts, df_ofcl_forecast])\n",
    "        except Exception as e:\n",
    "            print(\"Failed to parse forecast:\", e)\n",
    "            traceback.print_exc(limit=None, file=None, chain=True)\n",
    "            print(\"\")\n",
    "            print(\"Text of forecast:\")\n",
    "            print(txt)\n",
    "    return df_ofcl_forecasts\n",
    "\n",
    "def get_df_nhc_error_table_from_csv(table_file_path):\n",
    "    df = pd.read_csv(table_file_path)\n",
    "    # Filter rows that have '%' in the first column\n",
    "    df = df[df.iloc[:, 0].str.contains('%')]\n",
    "\n",
    "    # Rename the first column to 'percentile' and reset the index\n",
    "    df = df.rename(columns={df.columns[0]: 'percentile'}).reset_index(drop=True)\n",
    "\n",
    "    # Convert the 'percentile' column to float (remove '%' and divide by 100)\n",
    "    df['percentile'] = df['percentile'].str.rstrip('%')\n",
    "\n",
    "    # Convert the values in the DataFrame to integers\n",
    "    df.iloc[1:] = df.iloc[1:].astype(float)\n",
    "        \n",
    "    # Check for columns with NaN values\n",
    "    columns_with_nan = df.columns[df.isna().any()].tolist()\n",
    "\n",
    "    # Drop columns with NaN values\n",
    "    df = df.drop(columns=columns_with_nan)\n",
    "\n",
    "    df['percentile'] = df['percentile'].astype(float) / 100.0\n",
    "\n",
    "    # Set the index name to None\n",
    "    df.index.name = None\n",
    "    return df\n",
    "\n",
    "def get_df_with_deltas(df_no_deltas):\n",
    "    df_ofcl_forecasts_with_deltas = df_no_deltas.copy(deep=True)\n",
    "    df_ofcl_forecasts_with_deltas = df_ofcl_forecasts_with_deltas.reset_index(drop=True)\n",
    "    df_ofcl_forecasts_with_deltas['INIT_VMAX_KT'] = np.nan\n",
    "    df_ofcl_forecasts_with_deltas['DELTA_VMAX_KT'] = np.nan\n",
    "    df_ofcl_forecasts_with_deltas['RI'] = False\n",
    "\n",
    "    # SET INIT_MAX_KT FOR EACH ADVISORY\n",
    "    for i in range(0, len(df_ofcl_forecasts_with_deltas)):\n",
    "        storm_id = df_ofcl_forecasts_with_deltas.iloc[i]['ATCF_ID']\n",
    "        init_time = df_ofcl_forecasts_with_deltas.iloc[i]['INIT_TIME_UTC']\n",
    "        tau = df_ofcl_forecasts_with_deltas.iloc[i]['TAU']\n",
    "        if tau == 0:\n",
    "            init_vmax = df_ofcl_forecasts_with_deltas.iloc[i]['VMAX_KT']\n",
    "        else:\n",
    "            init_vmax = df_ofcl_forecasts_with_deltas.loc[\n",
    "                (df_ofcl_forecasts_with_deltas['ATCF_ID'] == storm_id) &\n",
    "                (df_ofcl_forecasts_with_deltas['INIT_TIME_UTC'] == init_time) &\n",
    "                (df_ofcl_forecasts_with_deltas['TAU'] == 0), 'VMAX_KT'\n",
    "            ].values[0]\n",
    "        df_ofcl_forecasts_with_deltas.at[i, 'INIT_VMAX_KT'] = init_vmax\n",
    "\n",
    "    # CALCULATE DELTA_VMAX_KT\n",
    "\n",
    "    df_ofcl_forecasts_with_deltas['DELTA_VMAX_KT'] = df_ofcl_forecasts_with_deltas.apply(\n",
    "        lambda x: x['VMAX_KT'] - x['INIT_VMAX_KT'], axis=1\n",
    "    )\n",
    "\n",
    "    # CALCULATE RI (for tau = 12h to 72h)\n",
    "    df_ofcl_forecasts_with_deltas['RI'] = df_ofcl_forecasts_with_deltas.apply(\n",
    "        lambda x: is_rapid_intensification(x['TAU'], x['DELTA_VMAX_KT']), axis=1\n",
    "    )\n",
    "    return df_ofcl_forecasts_with_deltas\n",
    "\n",
    "# calculate VMAX_ERROR: valid vmax - init vmax (from more recent advisory)\n",
    "def get_df_with_vmax_error(df_with_deltas):\n",
    "    df_ofcl_forecasts_errors = df_with_deltas.copy(deep=True)\n",
    "    df_ofcl_forecasts_errors['VMAX_ERROR'] = np.nan\n",
    "    # SET INIT_MAX_KT FOR EACH ADVISORY\n",
    "    for i in range(0, len(df_ofcl_forecasts_errors)):\n",
    "        storm_id = df_ofcl_forecasts_errors.iloc[i]['ATCF_ID']\n",
    "        valid_time = df_ofcl_forecasts_errors.iloc[i]['VALID_TIME_UTC']\n",
    "        valid_vmax = df_ofcl_forecasts_errors.iloc[i]['VMAX_KT']\n",
    "        init_vmax_row = df_ofcl_forecasts_errors.loc[\n",
    "                (df_ofcl_forecasts_errors['ATCF_ID'] == storm_id) &\n",
    "                (df_ofcl_forecasts_errors['INIT_TIME_UTC'] == valid_time) &\n",
    "                (df_ofcl_forecasts_errors['TAU'] == 0), 'VMAX_KT'\n",
    "        ]\n",
    "        if not init_vmax_row.empty:\n",
    "            init_vmax = init_vmax_row.values[0]\n",
    "            vmax_error = valid_vmax - init_vmax\n",
    "            df_ofcl_forecasts_errors.at[i, 'VMAX_ERROR'] = vmax_error\n",
    "    return df_ofcl_forecasts_errors\n",
    "        \n",
    "def get_df_nhc_error_tables():\n",
    "    df_error_tables = {}\n",
    "    for table_file_path in nhc_error_tables:\n",
    "        table_num = int(re.findall(r'table(\\d+).csv', table_file_path)[0])\n",
    "        df_error_table = get_df_nhc_error_table_from_csv(table_file_path)\n",
    "        df_error_tables[table_num] = df_error_table\n",
    "    return df_error_tables\n",
    "\n",
    "def get_table_basin_offset(basin):\n",
    "    table_offset = 0\n",
    "    if basin == 'AT' or basin == 'AL':\n",
    "        table_offset = 0\n",
    "    elif basin == 'EP':\n",
    "        table_offset = 5\n",
    "    return table_offset\n",
    "\n",
    "# table_num is indexed 1 (1-10), while the list for dfs is indexed 0 (0-4)\n",
    "# also, table_num is the full set (1-10), while the dfs list index is modulo 5, since it includes both basins\n",
    "def get_dfs_index_from_table_num(table_num):\n",
    "    return ((table_num - 1) % 5)\n",
    "\n",
    "# takes a list of intensity errors and calculates the percentile for a given tau\n",
    "# returns a dict, with a tuple of the range of probabilities for each given intensity error\n",
    "#    (if <0.01 or >.99, returns a tuple of (0,0) or (1,1) respectively)\n",
    "# returns None if tau is not in table\n",
    "def get_percentiles_for_intensity_errors(tau, basin, intensity_errors, basin_table_num=5):\n",
    "    # TODO: Haven't done CP tables yet\n",
    "    if basin == 'CP':\n",
    "        return None\n",
    "    table_offset = get_table_basin_offset(basin)\n",
    "    df_base_prob = df_error_tables[basin_table_num + table_offset]\n",
    "    tau_str = str(tau)\n",
    "    if tau_str in df_base_prob.columns:\n",
    "        prob_tuples_by_intensity_errors = {}\n",
    "        for intensity_error in intensity_errors:\n",
    "            prob_tuple = find_percentile_range(df_base_prob, intensity_error, tau_str)\n",
    "            prob_tuples_by_intensity_errors[intensity_error] = prob_tuple\n",
    "        return prob_tuples_by_intensity_errors    \n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "# returns a tuple (if outside the range return (0,0.1) or (0.99,1.0)\n",
    "# this uses 'intensity' var shorthand for forecast (valid) intensity error from init (valid intensity - init intensity)\n",
    "def find_percentile_range(df, intensity, tau):\n",
    "    # Find the corresponding 'percentile' values\n",
    "    probabilities = df['percentile']\n",
    "    values = df[tau]\n",
    "\n",
    "    # Check if there are values equal to the given intensity\n",
    "    equal_intensity = probabilities[values == intensity]\n",
    "\n",
    "    if not equal_intensity.empty:\n",
    "        # Values equal to the intensity are found\n",
    "        return (min(equal_intensity), max(equal_intensity))\n",
    "    else:\n",
    "        \n",
    "        # Check if the column is increasing or decreasing\n",
    "        increasing = values.is_monotonic_increasing\n",
    "        decreasing = values.is_monotonic_decreasing\n",
    "        \n",
    "        # Values equal to the intensity are not found\n",
    "        min_prob = probabilities[values <= intensity]\n",
    "        max_prob = probabilities[values >= intensity]\n",
    "\n",
    "        if min_prob.empty:\n",
    "            # Intensity is greater than the maximum value in the column\n",
    "            if increasing:\n",
    "                return (0.0, 0.01)\n",
    "            elif decreasing:\n",
    "                return (0, 0.1)\n",
    "            else:\n",
    "                # should never happen\n",
    "                return None\n",
    "        elif max_prob.empty:\n",
    "            # Intensity is less than the minimum value in the column\n",
    "            if increasing:\n",
    "                return (0.99, 1.0)\n",
    "            elif decreasing:\n",
    "                return (0, 0.1)\n",
    "            else:\n",
    "                # should never happen\n",
    "                return None\n",
    "        else:\n",
    "            # Intensity is between the minimum and maximum values in the column\n",
    "            return (max(min_prob), min(max_prob))\n",
    "\n",
    "# calculate probs from percentiles and returns dfs for each table\n",
    "def get_dfs_with_probs(df_ofcl_forecasts):\n",
    "    df_ofcl_forecasts_copy = df_ofcl_forecasts.copy(deep=True)\n",
    "    # calculate the deltas\n",
    "    df_ofcl_forecasts_with_deltas = get_df_with_deltas(df_ofcl_forecasts_copy)\n",
    "    # calculate the vmax errors using older forecasts\n",
    "    df_ofcl_forecasts_errors = get_df_with_vmax_error(df_ofcl_forecasts_with_deltas)\n",
    "    \n",
    "    # calculate probs for each basin table (1-5 is for AL 1-5 and EP 6-10)\n",
    "    dfs_with_probs = []\n",
    "    for basin_table in range(1,6):\n",
    "        df_with_probs = df_ofcl_forecasts_errors.copy(deep=True)\n",
    "\n",
    "        df_ofcl_forecasts_min_max_cat_percentiles = df_with_probs.copy(deep=True)\n",
    "        for cat_name in cat_min_max_kt.keys():\n",
    "            df_ofcl_forecasts_min_max_cat_percentiles[f'{cat_name}_L'] = np.nan\n",
    "            df_ofcl_forecasts_min_max_cat_percentiles[f'{cat_name}_U'] = np.nan\n",
    "\n",
    "        df_ofcl_forecasts_min_max_cat_percentiles = df_with_probs.copy(deep=True)\n",
    "        for p_cat_label in p_cat_labels.values():\n",
    "            df_ofcl_forecasts_min_max_cat_percentiles[p_cat_label] = np.nan\n",
    "\n",
    "        for i in range(0, len(df_ofcl_forecasts_min_max_cat_percentiles)):\n",
    "            storm_id = df_ofcl_forecasts_min_max_cat_percentiles.iloc[i]['ATCF_ID']\n",
    "            storm_basin = df_ofcl_forecasts_min_max_cat_percentiles.iloc[i]['BASIN']\n",
    "            valid_time = df_ofcl_forecasts_min_max_cat_percentiles.iloc[i]['VALID_TIME_UTC']\n",
    "            tau = df_ofcl_forecasts_min_max_cat_percentiles.iloc[i]['TAU']\n",
    "            valid_vmax = df_ofcl_forecasts_min_max_cat_percentiles.iloc[i]['VMAX_KT']\n",
    "\n",
    "            if np.isnan(valid_vmax):\n",
    "                continue\n",
    "\n",
    "            vmax_errors = []\n",
    "            for cat_name, cat_vmax in cat_min_max_kt.items():\n",
    "                vmax_error = valid_vmax - cat_vmax\n",
    "                vmax_errors.append(vmax_error)\n",
    "            percentiles = get_percentiles_for_intensity_errors(tau, storm_basin, vmax_errors)\n",
    "            percentiles_values = list(percentiles.values())\n",
    "            if percentiles:\n",
    "                for x, cat_name in enumerate(cat_min_max_kt.keys()):\n",
    "                    percentile_L, percentile_U = percentiles_values[x]\n",
    "                    df_ofcl_forecasts_min_max_cat_percentiles.at[i, f'{cat_name}_L'] = percentile_L\n",
    "                    df_ofcl_forecasts_min_max_cat_percentiles.at[i, f'{cat_name}_U'] = percentile_U\n",
    "            for cat, [cat_min, cat_max] in categories_kt.items():\n",
    "                cat_min_L_col = f'{cat}_MIN_L'\n",
    "                cat_min_U_col = f'{cat}_MIN_U'\n",
    "                cat_max_L_col = f'{cat}_MAX_L'\n",
    "                cat_max_U_col = f'{cat}_MAX_U'\n",
    "\n",
    "                cat_min_L = df_ofcl_forecasts_min_max_cat_percentiles.iloc[i][cat_min_L_col]\n",
    "                cat_min_U = df_ofcl_forecasts_min_max_cat_percentiles.iloc[i][cat_min_U_col]\n",
    "                cat_max_L = df_ofcl_forecasts_min_max_cat_percentiles.iloc[i][cat_max_L_col]\n",
    "                cat_max_U = df_ofcl_forecasts_min_max_cat_percentiles.iloc[i][cat_max_U_col]\n",
    "                min_percentile = min(cat_min_L, cat_min_U, cat_max_L, cat_max_U)\n",
    "                max_percentile = max(cat_min_L, cat_min_U, cat_max_L, cat_max_U)\n",
    "                prob = max_percentile - min_percentile\n",
    "                p_cat_label = p_cat_labels[cat]\n",
    "                df_with_probs.at[i, p_cat_label] = prob\n",
    "\n",
    "\n",
    "        dfs_with_probs.append(df_with_probs)\n",
    "\n",
    "    return dfs_with_probs\n",
    "\n",
    "def get_df_probs_ri_adjusted(dfs):\n",
    "    df_probs_ri = dfs[4].copy(deep=True)\n",
    "\n",
    "    for i in range(0, len(df_probs_ri)):\n",
    "        ri = df_probs_ri.iloc[i]['RI']\n",
    "        delta_vmax = df_probs_ri.iloc[i]['DELTA_VMAX_KT']\n",
    "        tau = df_probs_ri.iloc[i]['TAU']\n",
    "        if np.isnan(delta_vmax):\n",
    "            # don't calculate on rows where there are no probabilities\n",
    "            continue\n",
    "        \n",
    "        storm_id = df_probs_ri.iloc[i]['ATCF_ID']\n",
    "        storm_basin = df_probs_ri.iloc[i]['BASIN']\n",
    "        valid_time = df_probs_ri.iloc[i]['VALID_TIME_UTC']\n",
    "\n",
    "        if tau > max_tau_by_basin[storm_basin]:\n",
    "            # too few samples\n",
    "            continue\n",
    "\n",
    "        if ri:\n",
    "            ri_key = 'RI'\n",
    "        else:\n",
    "            ri_key = 'NO_RI'\n",
    "        \n",
    "        ri_weights = adjust_RI_weights[storm_basin][ri_key]\n",
    "        p_cat_column_names = list(p_cat_labels.values())\n",
    "        for p_cat_column_name in p_cat_column_names:\n",
    "            p = 0\n",
    "            for table_num, table_weight in ri_weights:\n",
    "                dfs_index = get_dfs_index_from_table_num(table_num)\n",
    "                table_p = dfs[dfs_index].iloc[i][p_cat_column_name]\n",
    "                ri_adj_p = table_weight * table_p\n",
    "                p += ri_adj_p\n",
    "            if not np.isnan(p):\n",
    "                prev_p = df_probs_ri.iloc[i][p_cat_column_name]\n",
    "                df_probs_ri.at[i, p_cat_column_name] = p\n",
    "                delta_p = p - prev_p\n",
    "                if (np.abs(delta_p) > 0.01):\n",
    "                    print(f\"{storm_id}\")\n",
    "                    print(f\"  New adjust p: {p:.4f}\")\n",
    "                    print(f\"    prev p: {prev_p:.4f}\")\n",
    "                    print(f\"    delta p: {delta_p:.4f}\")\n",
    "    \n",
    "    return df_probs_ri\n",
    "    \n",
    "def is_rapid_intensification(valid_h, intensity_change):\n",
    "    ri = False\n",
    "    if np.isnan(intensity_change):\n",
    "        return ri\n",
    "    if valid_h == 0:\n",
    "        # don't do RI for base time (000h)\n",
    "        return ri\n",
    "    # https://journals.ametsoc.org/view/journals/wefo/35/6/WAF-D-19-0253.1.xml#bib15\n",
    "    # 'RI is therefore defined as an increase of at least 20 kt in 12 h, 30 kt in 24 h, 45 kt in 36 h, and 55 kt in 48 h'\n",
    "    rapid_intensification_threshold = np.NaN\n",
    "    if valid_h <= 12:\n",
    "        rapid_intensification_threshold = 20\n",
    "    elif valid_h <= 24:\n",
    "        rapid_intensification_threshold = 30\n",
    "    elif valid_h <= 36:\n",
    "        rapid_intensification_threshold = 45\n",
    "    elif valid_h <= 48:\n",
    "        rapid_intensification_threshold = 55\n",
    "    elif valid_h <= 60:\n",
    "        rapid_intensification_threshold = 60\n",
    "    elif valid_h <= 72:\n",
    "        rapid_intensification_threshold = 65\n",
    "    # only consider rapid intensification for the above periods\n",
    "    if np.isnan(rapid_intensification_threshold):\n",
    "        return ri\n",
    "    if (intensity_change >= rapid_intensification_threshold):\n",
    "        ri = True\n",
    "    return ri\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "26eb608d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_error_tables = get_df_nhc_error_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "3dc9e512",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ofcl_forecasts = get_last_forecast_dfs()\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d0c074",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "7e712136",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_with_probs = get_dfs_with_probs(df_ofcl_forecasts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "711ab932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP162023\n",
      "  New adjust p: 0.9504\n",
      "    prev p: 0.9900\n",
      "    delta p: -0.0396\n",
      "EP172023\n",
      "  New adjust p: 0.7584\n",
      "    prev p: 0.7900\n",
      "    delta p: -0.0316\n",
      "EP172023\n",
      "  New adjust p: 0.6624\n",
      "    prev p: 0.6900\n",
      "    delta p: -0.0276\n",
      "EP172023\n",
      "  New adjust p: 0.4320\n",
      "    prev p: 0.4500\n",
      "    delta p: -0.0180\n",
      "EP172023\n",
      "  New adjust p: 0.2688\n",
      "    prev p: 0.2800\n",
      "    delta p: -0.0112\n",
      "EP172023\n",
      "  New adjust p: 0.4224\n",
      "    prev p: 0.4400\n",
      "    delta p: -0.0176\n",
      "EP172023\n",
      "  New adjust p: 0.2976\n",
      "    prev p: 0.3100\n",
      "    delta p: -0.0124\n",
      "EP172023\n",
      "  New adjust p: 0.3936\n",
      "    prev p: 0.4100\n",
      "    delta p: -0.0164\n",
      "EP172023\n",
      "  New adjust p: 0.5664\n",
      "    prev p: 0.5900\n",
      "    delta p: -0.0236\n",
      "EP172023\n",
      "  New adjust p: 0.4800\n",
      "    prev p: 0.5000\n",
      "    delta p: -0.0200\n",
      "EP132023\n",
      "  New adjust p: 0.7680\n",
      "    prev p: 0.8000\n",
      "    delta p: -0.0320\n",
      "EP132023\n",
      "  New adjust p: 0.8064\n",
      "    prev p: 0.8400\n",
      "    delta p: -0.0336\n",
      "EP132023\n",
      "  New adjust p: 0.7584\n",
      "    prev p: 0.7900\n",
      "    delta p: -0.0316\n",
      "EP142023\n",
      "  New adjust p: 0.9216\n",
      "    prev p: 0.9600\n",
      "    delta p: -0.0384\n",
      "EP142023\n",
      "  New adjust p: 0.8064\n",
      "    prev p: 0.8400\n",
      "    delta p: -0.0336\n",
      "EP142023\n",
      "  New adjust p: 0.6912\n",
      "    prev p: 0.7200\n",
      "    delta p: -0.0288\n",
      "EP142023\n",
      "  New adjust p: 0.6144\n",
      "    prev p: 0.6400\n",
      "    delta p: -0.0256\n",
      "EP152023\n",
      "  New adjust p: 0.7680\n",
      "    prev p: 0.8000\n",
      "    delta p: -0.0320\n",
      "EP032023\n",
      "  New adjust p: 0.8352\n",
      "    prev p: 0.8700\n",
      "    delta p: -0.0348\n",
      "EP032023\n",
      "  New adjust p: 0.6336\n",
      "    prev p: 0.6600\n",
      "    delta p: -0.0264\n",
      "EP032023\n",
      "  New adjust p: 0.3168\n",
      "    prev p: 0.3300\n",
      "    delta p: -0.0132\n",
      "EP032023\n",
      "  New adjust p: 0.5664\n",
      "    prev p: 0.5900\n",
      "    delta p: -0.0236\n",
      "EP032023\n",
      "  New adjust p: 0.3456\n",
      "    prev p: 0.3600\n",
      "    delta p: -0.0144\n",
      "EP052023\n",
      "  New adjust p: 0.7584\n",
      "    prev p: 0.7900\n",
      "    delta p: -0.0316\n",
      "EP052023\n",
      "  New adjust p: 0.6624\n",
      "    prev p: 0.6900\n",
      "    delta p: -0.0276\n",
      "EP052023\n",
      "  New adjust p: 0.3648\n",
      "    prev p: 0.3800\n",
      "    delta p: -0.0152\n",
      "EP052023\n",
      "  New adjust p: 0.3840\n",
      "    prev p: 0.4000\n",
      "    delta p: -0.0160\n",
      "EP052023\n",
      "  New adjust p: 0.4224\n",
      "    prev p: 0.4400\n",
      "    delta p: -0.0176\n",
      "EP052023\n",
      "  New adjust p: 0.2976\n",
      "    prev p: 0.3100\n",
      "    delta p: -0.0124\n",
      "EP052023\n",
      "  New adjust p: 0.3936\n",
      "    prev p: 0.4100\n",
      "    delta p: -0.0164\n",
      "EP052023\n",
      "  New adjust p: 0.4896\n",
      "    prev p: 0.5100\n",
      "    delta p: -0.0204\n",
      "EP052023\n",
      "  New adjust p: 0.2688\n",
      "    prev p: 0.2800\n",
      "    delta p: -0.0112\n",
      "EP052023\n",
      "  New adjust p: 0.4608\n",
      "    prev p: 0.4800\n",
      "    delta p: -0.0192\n",
      "EP082023\n",
      "  New adjust p: 0.7680\n",
      "    prev p: 0.8000\n",
      "    delta p: -0.0320\n",
      "EP082023\n",
      "  New adjust p: 0.6336\n",
      "    prev p: 0.6600\n",
      "    delta p: -0.0264\n",
      "EP082023\n",
      "  New adjust p: 0.3168\n",
      "    prev p: 0.3300\n",
      "    delta p: -0.0132\n",
      "EP122023\n",
      "  New adjust p: 0.7680\n",
      "    prev p: 0.8000\n",
      "    delta p: -0.0320\n",
      "EP122023\n",
      "  New adjust p: 0.8064\n",
      "    prev p: 0.8400\n",
      "    delta p: -0.0336\n",
      "EP052023\n",
      "  New adjust p: 0.9408\n",
      "    prev p: 0.9800\n",
      "    delta p: -0.0392\n",
      "EP052023\n",
      "  New adjust p: 0.3168\n",
      "    prev p: 0.3300\n",
      "    delta p: -0.0132\n",
      "EP052023\n",
      "  New adjust p: 0.5856\n",
      "    prev p: 0.6100\n",
      "    delta p: -0.0244\n",
      "EP052023\n",
      "  New adjust p: 0.4320\n",
      "    prev p: 0.4500\n",
      "    delta p: -0.0180\n",
      "EP052023\n",
      "  New adjust p: 0.3264\n",
      "    prev p: 0.3400\n",
      "    delta p: -0.0136\n",
      "EP052023\n",
      "  New adjust p: 0.3552\n",
      "    prev p: 0.3700\n",
      "    delta p: -0.0148\n",
      "EP052023\n",
      "  New adjust p: 0.3072\n",
      "    prev p: 0.3200\n",
      "    delta p: -0.0128\n",
      "EP052023\n",
      "  New adjust p: 0.3360\n",
      "    prev p: 0.3500\n",
      "    delta p: -0.0140\n",
      "EP052023\n",
      "  New adjust p: 0.3168\n",
      "    prev p: 0.3300\n",
      "    delta p: -0.0132\n",
      "EP052023\n",
      "  New adjust p: 0.3264\n",
      "    prev p: 0.3400\n",
      "    delta p: -0.0136\n"
     ]
    }
   ],
   "source": [
    "df_ri_adj = get_df_probs_ri_adjusted(dfs_with_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "43cd1573",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_percentiles_for_intensity_errors(12, 'AT', [-6, -5, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "4dd4dfa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================================\n",
      "NO RI ADJ. PROBABILITIES (CALCULATED USING PERCENTILES FROM 1981-2022)\n",
      "=======================================================================\n",
      "Norma a hurricane?\n",
      "2023-10-19 18Z :  98.1 %\n",
      "2023-10-20 06Z :  98.0 %\n",
      "2023-10-20 18Z :  97.9 %\n",
      "2023-10-21 06Z :  84.5 %\n",
      "2023-10-21 18Z :  65.6 %\n",
      "2023-10-22 06Z :  31.6 %\n",
      "2023-10-22 18Z :  24.7 %\n",
      "2023-10-23 18Z :   9.5 %\n",
      "2023-10-24 18Z :   nan %\n",
      "\n",
      "Norma a major hurricane?\n",
      "2023-10-19 18Z :  96.1 %\n",
      "2023-10-20 06Z :  84.2 %\n",
      "2023-10-20 18Z :  38.1 %\n",
      "2023-10-21 06Z :  14.4 %\n",
      "2023-10-21 18Z :   9.4 %\n",
      "2023-10-22 06Z :   4.1 %\n",
      "2023-10-22 18Z :   4.1 %\n",
      "2023-10-23 18Z :   3.2 %\n",
      "2023-10-24 18Z :   nan %\n",
      "\n",
      "Tammy a hurricane?\n",
      "2023-10-19 18Z :   4.8 %\n",
      "2023-10-20 06Z :   9.8 %\n",
      "2023-10-20 18Z :  25.0 %\n",
      "2023-10-21 06Z :  41.4 %\n",
      "2023-10-21 18Z :  54.6 %\n",
      "2023-10-22 06Z :  70.4 %\n",
      "2023-10-22 18Z :  74.0 %\n",
      "2023-10-23 18Z :  75.8 %\n",
      "2023-10-24 18Z :  75.5 %\n",
      "\n",
      "\n",
      "=======================================================================\n",
      "RI ADJ. PROBABILITIES (CALCULATED USING PERCENTILES FROM 1981-2022)\n",
      "=======================================================================\n",
      "Norma a hurricane?\n",
      "2023-10-19 18Z :  98.1 %\n",
      "2023-10-20 06Z :  98.0 %\n",
      "2023-10-20 18Z :  97.9 %\n",
      "2023-10-21 06Z :  84.5 %\n",
      "2023-10-21 18Z :  65.6 %\n",
      "2023-10-22 06Z :  31.6 %\n",
      "2023-10-22 18Z :  24.7 %\n",
      "2023-10-23 18Z :   9.5 %\n",
      "2023-10-24 18Z :   nan %\n",
      "\n",
      "Norma a major hurricane?\n",
      "2023-10-19 18Z :  96.1 %\n",
      "2023-10-20 06Z :  84.2 %\n",
      "2023-10-20 18Z :  38.1 %\n",
      "2023-10-21 06Z :  14.4 %\n",
      "2023-10-21 18Z :   9.4 %\n",
      "2023-10-22 06Z :   4.1 %\n",
      "2023-10-22 18Z :   4.1 %\n",
      "2023-10-23 18Z :   3.2 %\n",
      "2023-10-24 18Z :   nan %\n",
      "\n",
      "Tammy a hurricane?\n",
      "2023-10-19 18Z :   4.8 %\n",
      "2023-10-20 06Z :   9.8 %\n",
      "2023-10-20 18Z :  25.0 %\n",
      "2023-10-21 06Z :  41.4 %\n",
      "2023-10-21 18Z :  54.6 %\n",
      "2023-10-22 06Z :  70.4 %\n",
      "2023-10-22 18Z :  74.0 %\n",
      "2023-10-23 18Z :  75.8 %\n",
      "2023-10-24 18Z :  75.5 %\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# probabilities won't be normalized by category...\n",
    "# also <0.01 and 0.01 show as 0.01, just as 0.99 and >0.99 show as 0.99\n",
    "\n",
    "dfs_to_check = {\n",
    "    \"NO RI ADJ.\": dfs_with_probs[4],\n",
    "    \"RI ADJ.\": df_ri_adj\n",
    "}\n",
    "\n",
    "#df_check.loc[(df_check['NAME'] == 'NORMA')]\n",
    "for k, df_check in dfs_to_check.items():\n",
    "    print(\"=======================================================================\")\n",
    "    print(f\"{k} PROBABILITIES (CALCULATED USING PERCENTILES FROM 1981-2022)\")\n",
    "    print(\"=======================================================================\")\n",
    "    print(\"Norma a hurricane?\")\n",
    "    for idx in df_check.loc[(df_check['NAME'] == 'NORMA')].index:\n",
    "        cat1 = df_check.iloc[idx]['P(CAT1)']\n",
    "        cat2 = df_check.iloc[idx]['P(CAT2)']\n",
    "        cat3 = df_check.iloc[idx]['P(CAT3)']\n",
    "        cat4 = df_check.iloc[idx]['P(CAT4)']\n",
    "        cat5 = df_check.iloc[idx]['P(CAT5)']\n",
    "        ts = df_check.iloc[idx]['P(TS)']\n",
    "        td = df_check.iloc[idx]['P(TD)']\n",
    "        \n",
    "        hurricane = cat1+cat2+cat3+cat4+cat5\n",
    "        not_hurricane = ts+td\n",
    "        \n",
    "        norm = hurricane / (hurricane + not_hurricane)\n",
    "        \n",
    "        dt = df_check.iloc[idx]['VALID_TIME_UTC']\n",
    "        dt_str = dt.strftime(\"%Y-%m-%d %HZ\")\n",
    "        print(f\"{dt_str} : {norm*100: 5.1f} %\")\n",
    "\n",
    "    print(\"\")\n",
    "\n",
    "    print(\"Norma a major hurricane?\")\n",
    "    for idx in df_check.loc[(df_check['NAME'] == 'NORMA')].index:\n",
    "        cat1 = df_check.iloc[idx]['P(CAT1)']\n",
    "        cat2 = df_check.iloc[idx]['P(CAT2)']\n",
    "        cat3 = df_check.iloc[idx]['P(CAT3)']\n",
    "        cat4 = df_check.iloc[idx]['P(CAT4)']\n",
    "        cat5 = df_check.iloc[idx]['P(CAT5)']\n",
    "        ts = df_check.iloc[idx]['P(TS)']\n",
    "        td = df_check.iloc[idx]['P(TD)']\n",
    "        \n",
    "        major_hurricane = cat3+cat4+cat5\n",
    "        not_major_hurricane = ts+td+cat1+cat2\n",
    "        \n",
    "        norm = major_hurricane / (major_hurricane + not_major_hurricane)\n",
    "        \n",
    "        dt = df_check.iloc[idx]['VALID_TIME_UTC']\n",
    "        dt_str = dt.strftime(\"%Y-%m-%d %HZ\")\n",
    "        print(f\"{dt_str} : {norm*100: 5.1f} %\")\n",
    "\n",
    "    print(\"\")\n",
    "\n",
    "    print(\"Tammy a hurricane?\")\n",
    "    for idx in df_check.loc[(df_check['NAME'] == 'TAMMY')].index:\n",
    "        cat1 = df_check.iloc[idx]['P(CAT1)']\n",
    "        cat2 = df_check.iloc[idx]['P(CAT2)']\n",
    "        cat3 = df_check.iloc[idx]['P(CAT3)']\n",
    "        cat4 = df_check.iloc[idx]['P(CAT4)']\n",
    "        cat5 = df_check.iloc[idx]['P(CAT5)']\n",
    "        ts = df_check.iloc[idx]['P(TS)']\n",
    "        td = df_check.iloc[idx]['P(TD)']\n",
    "        \n",
    "        hurricane = cat1+cat2+cat3+cat4+cat5\n",
    "        not_hurricane = ts+td\n",
    "        \n",
    "        norm = hurricane / (hurricane + not_hurricane)\n",
    "        \n",
    "        dt = df_check.iloc[idx]['VALID_TIME_UTC']\n",
    "        dt_str = dt.strftime(\"%Y-%m-%d %HZ\")\n",
    "        print(f\"{dt_str} : {norm*100: 5.1f} %\")\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2a06ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "80476cb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9538546142141067"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# over a 24 hour period\n",
    "np.average([0.94,\n",
    "0.9693877551020408,\n",
    "0.9789473684210526,\n",
    "0.9270833333333333])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "8351bb34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9772310902516346"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average([\n",
    "0.9805825242718447,\n",
    "0.9800000000000001,\n",
    "0.9795918367346939,\n",
    "0.96875\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "25d9a4ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ATCF_ID</th>\n",
       "      <th>BASIN</th>\n",
       "      <th>STORM_NUM</th>\n",
       "      <th>SEASON</th>\n",
       "      <th>NAME</th>\n",
       "      <th>INIT_TIME_UTC</th>\n",
       "      <th>VALID_TIME_UTC</th>\n",
       "      <th>TAU</th>\n",
       "      <th>VMAX_KT</th>\n",
       "      <th>STORM_TYPE</th>\n",
       "      <th>...</th>\n",
       "      <th>DELTA_VMAX_KT</th>\n",
       "      <th>RI</th>\n",
       "      <th>VMAX_ERROR</th>\n",
       "      <th>P(TD)</th>\n",
       "      <th>P(TS)</th>\n",
       "      <th>P(CAT1)</th>\n",
       "      <th>P(CAT2)</th>\n",
       "      <th>P(CAT3)</th>\n",
       "      <th>P(CAT4)</th>\n",
       "      <th>P(CAT5)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>EP172023</td>\n",
       "      <td>EP</td>\n",
       "      <td>17</td>\n",
       "      <td>2023</td>\n",
       "      <td>NORMA</td>\n",
       "      <td>2023-10-19 18:00:00+00:00</td>\n",
       "      <td>2023-10-19 18:00:00+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>HURRICANE</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>EP172023</td>\n",
       "      <td>EP</td>\n",
       "      <td>17</td>\n",
       "      <td>2023</td>\n",
       "      <td>NORMA</td>\n",
       "      <td>2023-10-19 18:00:00+00:00</td>\n",
       "      <td>2023-10-20 06:00:00+00:00</td>\n",
       "      <td>12</td>\n",
       "      <td>105.0</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>EP172023</td>\n",
       "      <td>EP</td>\n",
       "      <td>17</td>\n",
       "      <td>2023</td>\n",
       "      <td>NORMA</td>\n",
       "      <td>2023-10-19 18:00:00+00:00</td>\n",
       "      <td>2023-10-20 18:00:00+00:00</td>\n",
       "      <td>24</td>\n",
       "      <td>95.0</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>-15.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>EP172023</td>\n",
       "      <td>EP</td>\n",
       "      <td>17</td>\n",
       "      <td>2023</td>\n",
       "      <td>NORMA</td>\n",
       "      <td>2023-10-19 18:00:00+00:00</td>\n",
       "      <td>2023-10-21 06:00:00+00:00</td>\n",
       "      <td>36</td>\n",
       "      <td>80.0</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>-30.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>EP172023</td>\n",
       "      <td>EP</td>\n",
       "      <td>17</td>\n",
       "      <td>2023</td>\n",
       "      <td>NORMA</td>\n",
       "      <td>2023-10-19 18:00:00+00:00</td>\n",
       "      <td>2023-10-21 18:00:00+00:00</td>\n",
       "      <td>48</td>\n",
       "      <td>70.0</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>-40.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>EP172023</td>\n",
       "      <td>EP</td>\n",
       "      <td>17</td>\n",
       "      <td>2023</td>\n",
       "      <td>NORMA</td>\n",
       "      <td>2023-10-19 18:00:00+00:00</td>\n",
       "      <td>2023-10-22 06:00:00+00:00</td>\n",
       "      <td>60</td>\n",
       "      <td>60.0</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>-50.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>EP172023</td>\n",
       "      <td>EP</td>\n",
       "      <td>17</td>\n",
       "      <td>2023</td>\n",
       "      <td>NORMA</td>\n",
       "      <td>2023-10-19 18:00:00+00:00</td>\n",
       "      <td>2023-10-22 18:00:00+00:00</td>\n",
       "      <td>72</td>\n",
       "      <td>50.0</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>-60.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>EP172023</td>\n",
       "      <td>EP</td>\n",
       "      <td>17</td>\n",
       "      <td>2023</td>\n",
       "      <td>NORMA</td>\n",
       "      <td>2023-10-19 18:00:00+00:00</td>\n",
       "      <td>2023-10-23 18:00:00+00:00</td>\n",
       "      <td>96</td>\n",
       "      <td>30.0</td>\n",
       "      <td>INLAND</td>\n",
       "      <td>...</td>\n",
       "      <td>-80.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>EP172023</td>\n",
       "      <td>EP</td>\n",
       "      <td>17</td>\n",
       "      <td>2023</td>\n",
       "      <td>NORMA</td>\n",
       "      <td>2023-10-19 18:00:00+00:00</td>\n",
       "      <td>2023-10-24 18:00:00+00:00</td>\n",
       "      <td>120</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DISSIPATED</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ATCF_ID BASIN  STORM_NUM SEASON   NAME             INIT_TIME_UTC  \\\n",
       "29  EP172023    EP         17   2023  NORMA 2023-10-19 18:00:00+00:00   \n",
       "30  EP172023    EP         17   2023  NORMA 2023-10-19 18:00:00+00:00   \n",
       "31  EP172023    EP         17   2023  NORMA 2023-10-19 18:00:00+00:00   \n",
       "32  EP172023    EP         17   2023  NORMA 2023-10-19 18:00:00+00:00   \n",
       "33  EP172023    EP         17   2023  NORMA 2023-10-19 18:00:00+00:00   \n",
       "34  EP172023    EP         17   2023  NORMA 2023-10-19 18:00:00+00:00   \n",
       "35  EP172023    EP         17   2023  NORMA 2023-10-19 18:00:00+00:00   \n",
       "36  EP172023    EP         17   2023  NORMA 2023-10-19 18:00:00+00:00   \n",
       "37  EP172023    EP         17   2023  NORMA 2023-10-19 18:00:00+00:00   \n",
       "\n",
       "              VALID_TIME_UTC  TAU  VMAX_KT  STORM_TYPE  ...  DELTA_VMAX_KT  \\\n",
       "29 2023-10-19 18:00:00+00:00    0    110.0   HURRICANE  ...            0.0   \n",
       "30 2023-10-20 06:00:00+00:00   12    105.0              ...           -5.0   \n",
       "31 2023-10-20 18:00:00+00:00   24     95.0              ...          -15.0   \n",
       "32 2023-10-21 06:00:00+00:00   36     80.0              ...          -30.0   \n",
       "33 2023-10-21 18:00:00+00:00   48     70.0              ...          -40.0   \n",
       "34 2023-10-22 06:00:00+00:00   60     60.0              ...          -50.0   \n",
       "35 2023-10-22 18:00:00+00:00   72     50.0              ...          -60.0   \n",
       "36 2023-10-23 18:00:00+00:00   96     30.0      INLAND  ...          -80.0   \n",
       "37 2023-10-24 18:00:00+00:00  120      NaN  DISSIPATED  ...            NaN   \n",
       "\n",
       "       RI  VMAX_ERROR  P(TD)  P(TS)  P(CAT1)  P(CAT2)  P(CAT3)  P(CAT4)  \\\n",
       "29  False         0.0   0.01   0.01     0.01     0.01     0.79     0.19   \n",
       "30  False         NaN   0.01   0.01     0.01     0.13     0.69     0.15   \n",
       "31  False         NaN   0.01   0.01     0.13     0.45     0.28     0.08   \n",
       "32  False         NaN   0.01   0.14     0.44     0.24     0.09     0.04   \n",
       "33  False         NaN   0.02   0.31     0.41     0.13     0.05     0.03   \n",
       "34  False         NaN   0.08   0.59     0.21     0.06     0.02     0.01   \n",
       "35  False         NaN   0.23   0.50     0.15     0.05     0.02     0.01   \n",
       "36  False         NaN   0.55   0.31     0.05     0.01     0.01     0.01   \n",
       "37  False         NaN    NaN    NaN      NaN      NaN      NaN      NaN   \n",
       "\n",
       "    P(CAT5)  \n",
       "29     0.01  \n",
       "30     0.01  \n",
       "31     0.01  \n",
       "32     0.01  \n",
       "33     0.01  \n",
       "34     0.01  \n",
       "35     0.01  \n",
       "36     0.01  \n",
       "37      NaN  \n",
       "\n",
       "[9 rows x 21 columns]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs_with_probs[4][dfs_with_probs[4]['ATCF_ID'] == 'EP172023']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "6f05cb4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ATCF_ID</th>\n",
       "      <th>BASIN</th>\n",
       "      <th>STORM_NUM</th>\n",
       "      <th>SEASON</th>\n",
       "      <th>NAME</th>\n",
       "      <th>INIT_TIME_UTC</th>\n",
       "      <th>VALID_TIME_UTC</th>\n",
       "      <th>TAU</th>\n",
       "      <th>VMAX_KT</th>\n",
       "      <th>STORM_TYPE</th>\n",
       "      <th>...</th>\n",
       "      <th>DELTA_VMAX_KT</th>\n",
       "      <th>RI</th>\n",
       "      <th>VMAX_ERROR</th>\n",
       "      <th>P(TD)</th>\n",
       "      <th>P(TS)</th>\n",
       "      <th>P(CAT1)</th>\n",
       "      <th>P(CAT2)</th>\n",
       "      <th>P(CAT3)</th>\n",
       "      <th>P(CAT4)</th>\n",
       "      <th>P(CAT5)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>EP172023</td>\n",
       "      <td>EP</td>\n",
       "      <td>17</td>\n",
       "      <td>2023</td>\n",
       "      <td>NORMA</td>\n",
       "      <td>2023-10-19 18:00:00+00:00</td>\n",
       "      <td>2023-10-19 18:00:00+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>HURRICANE</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0096</td>\n",
       "      <td>0.0096</td>\n",
       "      <td>0.0096</td>\n",
       "      <td>0.0096</td>\n",
       "      <td>0.7584</td>\n",
       "      <td>0.1824</td>\n",
       "      <td>0.0096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>EP172023</td>\n",
       "      <td>EP</td>\n",
       "      <td>17</td>\n",
       "      <td>2023</td>\n",
       "      <td>NORMA</td>\n",
       "      <td>2023-10-19 18:00:00+00:00</td>\n",
       "      <td>2023-10-20 06:00:00+00:00</td>\n",
       "      <td>12</td>\n",
       "      <td>105.0</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0096</td>\n",
       "      <td>0.0096</td>\n",
       "      <td>0.0096</td>\n",
       "      <td>0.1248</td>\n",
       "      <td>0.6624</td>\n",
       "      <td>0.1440</td>\n",
       "      <td>0.0096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>EP172023</td>\n",
       "      <td>EP</td>\n",
       "      <td>17</td>\n",
       "      <td>2023</td>\n",
       "      <td>NORMA</td>\n",
       "      <td>2023-10-19 18:00:00+00:00</td>\n",
       "      <td>2023-10-20 18:00:00+00:00</td>\n",
       "      <td>24</td>\n",
       "      <td>95.0</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>-15.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0096</td>\n",
       "      <td>0.0096</td>\n",
       "      <td>0.1248</td>\n",
       "      <td>0.4320</td>\n",
       "      <td>0.2688</td>\n",
       "      <td>0.0768</td>\n",
       "      <td>0.0096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>EP172023</td>\n",
       "      <td>EP</td>\n",
       "      <td>17</td>\n",
       "      <td>2023</td>\n",
       "      <td>NORMA</td>\n",
       "      <td>2023-10-19 18:00:00+00:00</td>\n",
       "      <td>2023-10-21 06:00:00+00:00</td>\n",
       "      <td>36</td>\n",
       "      <td>80.0</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>-30.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0096</td>\n",
       "      <td>0.1344</td>\n",
       "      <td>0.4224</td>\n",
       "      <td>0.2304</td>\n",
       "      <td>0.0864</td>\n",
       "      <td>0.0384</td>\n",
       "      <td>0.0096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>EP172023</td>\n",
       "      <td>EP</td>\n",
       "      <td>17</td>\n",
       "      <td>2023</td>\n",
       "      <td>NORMA</td>\n",
       "      <td>2023-10-19 18:00:00+00:00</td>\n",
       "      <td>2023-10-21 18:00:00+00:00</td>\n",
       "      <td>48</td>\n",
       "      <td>70.0</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>-40.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0192</td>\n",
       "      <td>0.2976</td>\n",
       "      <td>0.3936</td>\n",
       "      <td>0.1248</td>\n",
       "      <td>0.0480</td>\n",
       "      <td>0.0288</td>\n",
       "      <td>0.0096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>EP172023</td>\n",
       "      <td>EP</td>\n",
       "      <td>17</td>\n",
       "      <td>2023</td>\n",
       "      <td>NORMA</td>\n",
       "      <td>2023-10-19 18:00:00+00:00</td>\n",
       "      <td>2023-10-22 06:00:00+00:00</td>\n",
       "      <td>60</td>\n",
       "      <td>60.0</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>-50.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0768</td>\n",
       "      <td>0.5664</td>\n",
       "      <td>0.2016</td>\n",
       "      <td>0.0576</td>\n",
       "      <td>0.0192</td>\n",
       "      <td>0.0096</td>\n",
       "      <td>0.0096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>EP172023</td>\n",
       "      <td>EP</td>\n",
       "      <td>17</td>\n",
       "      <td>2023</td>\n",
       "      <td>NORMA</td>\n",
       "      <td>2023-10-19 18:00:00+00:00</td>\n",
       "      <td>2023-10-22 18:00:00+00:00</td>\n",
       "      <td>72</td>\n",
       "      <td>50.0</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>-60.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2208</td>\n",
       "      <td>0.4800</td>\n",
       "      <td>0.1440</td>\n",
       "      <td>0.0480</td>\n",
       "      <td>0.0192</td>\n",
       "      <td>0.0096</td>\n",
       "      <td>0.0096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>EP172023</td>\n",
       "      <td>EP</td>\n",
       "      <td>17</td>\n",
       "      <td>2023</td>\n",
       "      <td>NORMA</td>\n",
       "      <td>2023-10-19 18:00:00+00:00</td>\n",
       "      <td>2023-10-23 18:00:00+00:00</td>\n",
       "      <td>96</td>\n",
       "      <td>30.0</td>\n",
       "      <td>INLAND</td>\n",
       "      <td>...</td>\n",
       "      <td>-80.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5500</td>\n",
       "      <td>0.3100</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>EP172023</td>\n",
       "      <td>EP</td>\n",
       "      <td>17</td>\n",
       "      <td>2023</td>\n",
       "      <td>NORMA</td>\n",
       "      <td>2023-10-19 18:00:00+00:00</td>\n",
       "      <td>2023-10-24 18:00:00+00:00</td>\n",
       "      <td>120</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DISSIPATED</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ATCF_ID BASIN  STORM_NUM SEASON   NAME             INIT_TIME_UTC  \\\n",
       "29  EP172023    EP         17   2023  NORMA 2023-10-19 18:00:00+00:00   \n",
       "30  EP172023    EP         17   2023  NORMA 2023-10-19 18:00:00+00:00   \n",
       "31  EP172023    EP         17   2023  NORMA 2023-10-19 18:00:00+00:00   \n",
       "32  EP172023    EP         17   2023  NORMA 2023-10-19 18:00:00+00:00   \n",
       "33  EP172023    EP         17   2023  NORMA 2023-10-19 18:00:00+00:00   \n",
       "34  EP172023    EP         17   2023  NORMA 2023-10-19 18:00:00+00:00   \n",
       "35  EP172023    EP         17   2023  NORMA 2023-10-19 18:00:00+00:00   \n",
       "36  EP172023    EP         17   2023  NORMA 2023-10-19 18:00:00+00:00   \n",
       "37  EP172023    EP         17   2023  NORMA 2023-10-19 18:00:00+00:00   \n",
       "\n",
       "              VALID_TIME_UTC  TAU  VMAX_KT  STORM_TYPE  ...  DELTA_VMAX_KT  \\\n",
       "29 2023-10-19 18:00:00+00:00    0    110.0   HURRICANE  ...            0.0   \n",
       "30 2023-10-20 06:00:00+00:00   12    105.0              ...           -5.0   \n",
       "31 2023-10-20 18:00:00+00:00   24     95.0              ...          -15.0   \n",
       "32 2023-10-21 06:00:00+00:00   36     80.0              ...          -30.0   \n",
       "33 2023-10-21 18:00:00+00:00   48     70.0              ...          -40.0   \n",
       "34 2023-10-22 06:00:00+00:00   60     60.0              ...          -50.0   \n",
       "35 2023-10-22 18:00:00+00:00   72     50.0              ...          -60.0   \n",
       "36 2023-10-23 18:00:00+00:00   96     30.0      INLAND  ...          -80.0   \n",
       "37 2023-10-24 18:00:00+00:00  120      NaN  DISSIPATED  ...            NaN   \n",
       "\n",
       "       RI  VMAX_ERROR   P(TD)   P(TS)  P(CAT1)  P(CAT2)  P(CAT3)  P(CAT4)  \\\n",
       "29  False         0.0  0.0096  0.0096   0.0096   0.0096   0.7584   0.1824   \n",
       "30  False         NaN  0.0096  0.0096   0.0096   0.1248   0.6624   0.1440   \n",
       "31  False         NaN  0.0096  0.0096   0.1248   0.4320   0.2688   0.0768   \n",
       "32  False         NaN  0.0096  0.1344   0.4224   0.2304   0.0864   0.0384   \n",
       "33  False         NaN  0.0192  0.2976   0.3936   0.1248   0.0480   0.0288   \n",
       "34  False         NaN  0.0768  0.5664   0.2016   0.0576   0.0192   0.0096   \n",
       "35  False         NaN  0.2208  0.4800   0.1440   0.0480   0.0192   0.0096   \n",
       "36  False         NaN  0.5500  0.3100   0.0500   0.0100   0.0100   0.0100   \n",
       "37  False         NaN     NaN     NaN      NaN      NaN      NaN      NaN   \n",
       "\n",
       "    P(CAT5)  \n",
       "29   0.0096  \n",
       "30   0.0096  \n",
       "31   0.0096  \n",
       "32   0.0096  \n",
       "33   0.0096  \n",
       "34   0.0096  \n",
       "35   0.0096  \n",
       "36   0.0100  \n",
       "37      NaN  \n",
       "\n",
       "[9 rows x 21 columns]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ri_adj.loc[df_ri_adj['ATCF_ID'] == 'EP172023']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "50889369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ATCF_ID</th>\n",
       "      <th>BASIN</th>\n",
       "      <th>STORM_NUM</th>\n",
       "      <th>SEASON</th>\n",
       "      <th>NAME</th>\n",
       "      <th>INIT_TIME_UTC</th>\n",
       "      <th>VALID_TIME_UTC</th>\n",
       "      <th>TAU</th>\n",
       "      <th>VMAX_KT</th>\n",
       "      <th>STORM_TYPE</th>\n",
       "      <th>...</th>\n",
       "      <th>DELTA_VMAX_KT</th>\n",
       "      <th>RI</th>\n",
       "      <th>VMAX_ERROR</th>\n",
       "      <th>P(TD)</th>\n",
       "      <th>P(TS)</th>\n",
       "      <th>P(CAT1)</th>\n",
       "      <th>P(CAT2)</th>\n",
       "      <th>P(CAT3)</th>\n",
       "      <th>P(CAT4)</th>\n",
       "      <th>P(CAT5)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>AL202023</td>\n",
       "      <td>AL</td>\n",
       "      <td>20</td>\n",
       "      <td>2023</td>\n",
       "      <td>TAMMY</td>\n",
       "      <td>2023-10-19 18:00:00+00:00</td>\n",
       "      <td>2023-10-19 18:00:00+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>TROPICAL STORM</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>AL202023</td>\n",
       "      <td>AL</td>\n",
       "      <td>20</td>\n",
       "      <td>2023</td>\n",
       "      <td>TAMMY</td>\n",
       "      <td>2023-10-19 18:00:00+00:00</td>\n",
       "      <td>2023-10-20 06:00:00+00:00</td>\n",
       "      <td>12</td>\n",
       "      <td>50.0</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>AL202023</td>\n",
       "      <td>AL</td>\n",
       "      <td>20</td>\n",
       "      <td>2023</td>\n",
       "      <td>TAMMY</td>\n",
       "      <td>2023-10-19 18:00:00+00:00</td>\n",
       "      <td>2023-10-20 18:00:00+00:00</td>\n",
       "      <td>24</td>\n",
       "      <td>55.0</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>AL202023</td>\n",
       "      <td>AL</td>\n",
       "      <td>20</td>\n",
       "      <td>2023</td>\n",
       "      <td>TAMMY</td>\n",
       "      <td>2023-10-19 18:00:00+00:00</td>\n",
       "      <td>2023-10-21 06:00:00+00:00</td>\n",
       "      <td>36</td>\n",
       "      <td>60.0</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>AL202023</td>\n",
       "      <td>AL</td>\n",
       "      <td>20</td>\n",
       "      <td>2023</td>\n",
       "      <td>TAMMY</td>\n",
       "      <td>2023-10-19 18:00:00+00:00</td>\n",
       "      <td>2023-10-21 18:00:00+00:00</td>\n",
       "      <td>48</td>\n",
       "      <td>65.0</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>AL202023</td>\n",
       "      <td>AL</td>\n",
       "      <td>20</td>\n",
       "      <td>2023</td>\n",
       "      <td>TAMMY</td>\n",
       "      <td>2023-10-19 18:00:00+00:00</td>\n",
       "      <td>2023-10-22 06:00:00+00:00</td>\n",
       "      <td>60</td>\n",
       "      <td>70.0</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>AL202023</td>\n",
       "      <td>AL</td>\n",
       "      <td>20</td>\n",
       "      <td>2023</td>\n",
       "      <td>TAMMY</td>\n",
       "      <td>2023-10-19 18:00:00+00:00</td>\n",
       "      <td>2023-10-22 18:00:00+00:00</td>\n",
       "      <td>72</td>\n",
       "      <td>75.0</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>25.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>AL202023</td>\n",
       "      <td>AL</td>\n",
       "      <td>20</td>\n",
       "      <td>2023</td>\n",
       "      <td>TAMMY</td>\n",
       "      <td>2023-10-19 18:00:00+00:00</td>\n",
       "      <td>2023-10-23 18:00:00+00:00</td>\n",
       "      <td>96</td>\n",
       "      <td>75.0</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>25.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>AL202023</td>\n",
       "      <td>AL</td>\n",
       "      <td>20</td>\n",
       "      <td>2023</td>\n",
       "      <td>TAMMY</td>\n",
       "      <td>2023-10-19 18:00:00+00:00</td>\n",
       "      <td>2023-10-24 18:00:00+00:00</td>\n",
       "      <td>120</td>\n",
       "      <td>75.0</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>25.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ATCF_ID BASIN  STORM_NUM SEASON   NAME             INIT_TIME_UTC  \\\n",
       "18  AL202023    AL         20   2023  TAMMY 2023-10-19 18:00:00+00:00   \n",
       "19  AL202023    AL         20   2023  TAMMY 2023-10-19 18:00:00+00:00   \n",
       "20  AL202023    AL         20   2023  TAMMY 2023-10-19 18:00:00+00:00   \n",
       "21  AL202023    AL         20   2023  TAMMY 2023-10-19 18:00:00+00:00   \n",
       "22  AL202023    AL         20   2023  TAMMY 2023-10-19 18:00:00+00:00   \n",
       "23  AL202023    AL         20   2023  TAMMY 2023-10-19 18:00:00+00:00   \n",
       "24  AL202023    AL         20   2023  TAMMY 2023-10-19 18:00:00+00:00   \n",
       "25  AL202023    AL         20   2023  TAMMY 2023-10-19 18:00:00+00:00   \n",
       "26  AL202023    AL         20   2023  TAMMY 2023-10-19 18:00:00+00:00   \n",
       "\n",
       "              VALID_TIME_UTC  TAU  VMAX_KT      STORM_TYPE  ...  \\\n",
       "18 2023-10-19 18:00:00+00:00    0     50.0  TROPICAL STORM  ...   \n",
       "19 2023-10-20 06:00:00+00:00   12     50.0                  ...   \n",
       "20 2023-10-20 18:00:00+00:00   24     55.0                  ...   \n",
       "21 2023-10-21 06:00:00+00:00   36     60.0                  ...   \n",
       "22 2023-10-21 18:00:00+00:00   48     65.0                  ...   \n",
       "23 2023-10-22 06:00:00+00:00   60     70.0                  ...   \n",
       "24 2023-10-22 18:00:00+00:00   72     75.0                  ...   \n",
       "25 2023-10-23 18:00:00+00:00   96     75.0                  ...   \n",
       "26 2023-10-24 18:00:00+00:00  120     75.0                  ...   \n",
       "\n",
       "    DELTA_VMAX_KT     RI  VMAX_ERROR  P(TD)  P(TS)  P(CAT1)  P(CAT2)  P(CAT3)  \\\n",
       "18            0.0  False         0.0   0.01   0.98     0.01     0.01     0.01   \n",
       "19            0.0  False         NaN   0.02   0.90     0.06     0.01     0.01   \n",
       "20            5.0  False         NaN   0.03   0.72     0.20     0.02     0.01   \n",
       "21           10.0  False         NaN   0.03   0.55     0.32     0.05     0.02   \n",
       "22           15.0  False         NaN   0.02   0.42     0.38     0.10     0.03   \n",
       "23           20.0  False         NaN   0.01   0.28     0.52     0.12     0.03   \n",
       "24           25.0  False         NaN   0.02   0.23     0.42     0.16     0.08   \n",
       "25           25.0  False         NaN   0.02   0.21     0.41     0.18     0.08   \n",
       "26           25.0  False         NaN   0.03   0.20     0.38     0.17     0.10   \n",
       "\n",
       "    P(CAT4)  P(CAT5)  \n",
       "18     0.01     0.01  \n",
       "19     0.01     0.01  \n",
       "20     0.01     0.01  \n",
       "21     0.01     0.01  \n",
       "22     0.01     0.01  \n",
       "23     0.01     0.01  \n",
       "24     0.04     0.01  \n",
       "25     0.04     0.01  \n",
       "26     0.04     0.02  \n",
       "\n",
       "[9 rows x 21 columns]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ri_adj.loc[df_ri_adj['ATCF_ID'] == 'AL202023']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5200f4f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
